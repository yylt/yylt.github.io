[{"categories":["container","problem"],"content":"kubelet memroy leak","date":"2021-11-09","objectID":"/2021/11/memoryleakcontainerd/","tags":["containerd","kubelet"],"title":"记一次kubelet 内存泄漏","uri":"/2021/11/memoryleakcontainerd/"},{"categories":["container","problem"],"content":"背景 一次生产环境中，发现 kubelet 内存达到上 GB 以上，这个不符合平常使用情况，首先想到的是内存泄漏，那么肯定使用 pprof 以及调查为什么会产生内存泄漏 profile ","date":"2021-11-09","objectID":"/2021/11/memoryleakcontainerd/:0:0","tags":["containerd","kubelet"],"title":"记一次kubelet 内存泄漏","uri":"/2021/11/memoryleakcontainerd/"},{"categories":["container","problem"],"content":"基础 cpu /debug/pprof/profile，主要分析耗时和优化算法，得到 profile 文件 heap: /debug/pprof/heap，查看活动对象的内存分配情况，得到 profile 文件 threadcreate: /debug/pprof/threadcreate, 线程创建概况报告程序中导致创建新的操作系统线程的部分 goroutine： 报告所有当前 goroutine 的堆栈信息，没有 profile 文件 trace: /debug/pprof/trace 当前程序的执行跟踪，go tool trace 中使用 kubelet 查看平台的内存使用情况，如下 注：（以下数据非当时环境，是之后重新复现后取的，比发生泄漏环境要低的多) # top PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 31599 root 20 0 2877396 983.5m 66424 S 4.3 6.2 28:00.93 kubelet kubelet 数据默认打开 pprof， 通过 pprof 拿到数据 , 通过以下命令方式 # pprof -tls_ca {cafile} -tls_key {keyfile} -tls_cert {certfile} https://{host}:10250/debug/pprof/heap 拿到数据后，还可以拿到正常节点的数据做对比，如下 正常 kubelet 内存使用 不正常 kubelet 内存使用 ","date":"2021-11-09","objectID":"/2021/11/memoryleakcontainerd/:1:0","tags":["containerd","kubelet"],"title":"记一次kubelet 内存泄漏","uri":"/2021/11/memoryleakcontainerd/"},{"categories":["container","problem"],"content":"基础 描述下 kubelet 当前目录和会涉及的代码片段 pkg/kubelet/ ├── cri #cri接口 ├── server # http hanlder入口 ├── stats # 容器 cpu/memory，filesystem info 抓取 ├── kuberuntime # 桥接容器运行时 和 kubelet操作 └── cm #container manager缩写，控制器内容包括 cgroup，topology,device等 可以直接跨越到 cri/ 内查看 当前接口实现，未设置超时时间 func (r *remoteRuntimeService) ListContainerStats(filter *runtimeapi.ContainerStatsFilter) ([]*runtimeapi.ContainerStats, error) { // Do not set timeout, because writable layer stats collection takes time. // TODO(random-liu): Should we assume runtime should cache the result, and set timeout here? ctx, cancel := getContextWithCancel() defer cancel() // 这里未设置超时时间 resp, err := r.runtimeClient.ListContainerStats(ctx, \u0026runtimeapi.ListContainerStatsRequest{ Filter: filter, }) 当前调用链大致如下 , 可以看到及时客户端退出，但是 kubelet 到 containerd 的连接还是会保持，直到拿到数据为止 那么考虑就是如何复现这个问题，但通过测试代码 [1] 并未复现问题，这里其实一直是阻塞调查的地方，那么不妨换个思路，现继续挖下为什么 containerd 没有返回数据呢？ 到这里，其实可以发现 kubelet 都集中在 stats.ListPodStats 花费上，该调用会最终到 containerd 的 Stats 接口上，那么应该继续分析 containerd ，这应该是产生问题的根本原因 containerd 通过抓取 heap 和 goroutine 信息分析，而我们环境中 containerd 的 debug socket 已经打开，所以比较方便拿到 containerd profile 信息 # ctr pprof heap \u003e heap.profile # ctr pprof goroutines \u003e goroutine.profile 拿到数据后，还可以拿到正常节点的数据做对比 正常 containerd 内存使用 不正常 containerd 内存使用 看到的是 SPDY 内存消耗较多，我们知道 SPDY 是 HTTP/2 前身，主要用于流式连接，当前主要是 接口 exec, attach, portfoward 在接口上，以下是简单对 exec 的代码理解 , 然后使用了 exec 确能稳定复现问题，复现代码如下 #!/bin/bash i=0 NUM=$1 while [ \"$i\" -lt $NUM ]; do (kubectl exec -i test-74cf75b654-gw5hk -- ls /opt)\u0026 let \"i += 1\" done ","date":"2021-11-09","objectID":"/2021/11/memoryleakcontainerd/:2:0","tags":["containerd","kubelet"],"title":"记一次kubelet 内存泄漏","uri":"/2021/11/memoryleakcontainerd/"},{"categories":["container","problem"],"content":"exec 流程 流程需要从 kubelet 开始分析 , 通过前文中对 kubelet 目录的简单介绍，那么入口肯定是在 server 目录内，直接到主题 Exec，对应函数是 getExec(request *restful.Request, response *restful.Response) ， 行为简单描述如下 - 校验参数，通常 stdout/stderr 为 true，目前常用的是 -it，也就是需要配置 stdin 和 ttry 是否为 true - 查询 pod 当前是否支持 exec - 执行 GetExec 获取 url - 创建代理，转发 apiserver 数据 stdin 和 stdout cri 服务 主要分为两部分，重点描述 ServerExec - GetExec：创建 url 路由，增加 token host:port/exec/{token} - ServerExec： 创建 task 和 process，并绑定标准输入和输出等 ServerExec 升级 http 为 SPDY stream，看上去 spdy 是比较重，因为要至少建立三个 goroutine handler.waitForStreams(streamCh, expectedStreams, expired.C) 执行 Exec (在 streamRuntime 内)，等待 process 结束，或者上游退出 //创建Task， task实际是 containerd/containerd/task.go 类型 task, err := container.Task(ctx, nil) if err != nil { return nil, errors.Wrap(err, \"failed to load task\") } pspec := spec.Process pspec.Args = opts.cmd pspec.Terminal = opts.tty if opts.tty { oci.WithEnv([]string{\"TERM=xterm\"})(ctx, nil, nil, spec) } volatileRootDir := c.getVolatileContainerRootDir(id) var execIO *cio.ExecIO // 创建process, 实际是 containerd/containerd/process.go 类型 process, err := task.Exec(ctx, execID, pspec, func(id string) (containerdio.IO, error) { var err error execIO, err = cio.NewExecIO(id, volatileRootDir, opts.tty, opts.stdin != nil) return execIO, err }, ) // 获取 process 退出管道 exitCh, err := process.Wait(ctx) err := process.Start(ctx) // 将execIO 和 http 流绑定 // 内部会创建协程 stdin ，stdout, waitGroup attachDone := execIO.Attach(cio.AttachOptions{ Stdin: opts.stdin, Stdout: opts.stdout, Stderr: opts.stderr, Tty: opts.tty, StdinOnce: true, CloseStdin: func() error { return process.CloseIO(ctx, containerd.WithStdinCloser) }, }) select { case \u003c-execCtx.Done(): case exitRes := \u003c-exitCh: } 这里是阻塞在调用 exec 上，但是和 cpuAndMemoryStats 并无关系，如果是有关系，那么也只有在 shim 这一级，因为都要调用 shim 的 rpc 接口 shim 当前 runc 使用的 shim 是 containerd-shim-runc-v2, 在内置的 shim server 上有一个方便调试的方式，发送 USER1 信号 可以获取 goroutines 信息，具体代码如下 // runtime/v2/shim/shim_unix.go func setupDumpStacks(dump chan\u003c- os.Signal) { signal.Notify(dump, syscall.SIGUSR1) } // 发送 USER1 信号 kill -10 {pid} // 获取containerd 日志，获取BEGIN goroutine stack dump 日志 journalctl -eu containerd \u003econtaienrd.log 查看 shim 的 goroutines 信息，因为和 ttrpc 有关，那直接找 ttrpc 信息吧，可以发现以下信息，这里显然不应该有那么多 goroutine hang 在 vendor/github.com/containerd/ttrpc/server.go:444 行，那么继续看下 ttrpc 有关实现呢 $ cat shim.goroutine.profile |grep ttrpc/server | sort |uniq -c |sort 1 vendor/github.com/containerd/ttrpc/server.go:362 +0x149 1 vendor/github.com/containerd/ttrpc/server.go:404 +0x5ee 1 vendor/github.com/containerd/ttrpc/server.go:431 +0x41a 1 vendor/github.com/containerd/ttrpc/server.go:459 +0x6bd 1 vendor/github.com/containerd/ttrpc/server.go:87 +0x107 2 vendor/github.com/containerd/ttrpc/server.go:127 +0x2a7 2 vendor/github.com/containerd/ttrpc/server.go:332 +0x2ce 6 vendor/github.com/containerd/ttrpc/server.go:438 +0xf2 164 vendor/github.com/containerd/ttrpc/server.go:444 +0x245 169 vendor/github.com/containerd/ttrpc/server.go:434 +0x63f ","date":"2021-11-09","objectID":"/2021/11/memoryleakcontainerd/:3:0","tags":["containerd","kubelet"],"title":"记一次kubelet 内存泄漏","uri":"/2021/11/memoryleakcontainerd/"},{"categories":["container","problem"],"content":"ttrpc reqCh, respCh, msgCh 均为非缓存 channel 服务端大致如下 每创建一个连接， 都会产生 2 个协程来处理会话 recv 协程： 从客户端读取数据，并校验合法性写入 Channel 中，交给 worker 来处理 worker 协程：收到请求后，并发创建协程调用注册的服务 1.0.1 版本 客户端大致如下 和服务端类似，工作协程同时处理接收和发送请求， 重点是 发送和接收是在一个协程中处理，并通过内部 waitCall 来同步数据 发生死锁的情况是 客户端 阻塞在 send 过程，此时因为无法处理返回的 Resp 信息，继而导致服务端的应答数据阻塞在 net write buffer 中 什么情况会导致 send 阻塞，网络发送过程有以下情况 , 进程将数据拷贝到内核缓存区，之后由软中断发送出去，该控制写缓存大小为 tcp_wmem, 内核参数配置为 4096 16384 4194304 针对上述导致死锁的情况，有一个相关 patch[2] 解决，该 patch 修改方式如下图 1.1.0 版本 客户端 发送和接收 都通过不同的协程处理，不再出现竞争情况发生 参考 [1]. https://gist.github.com/yylt/0d3f2d554fa7eddd9cafe406ef0c9d75 [2]. https://github.com/containerd/ttrpc/pull/94 ","date":"2021-11-09","objectID":"/2021/11/memoryleakcontainerd/:4:0","tags":["containerd","kubelet"],"title":"记一次kubelet 内存泄漏","uri":"/2021/11/memoryleakcontainerd/"},{"categories":["container"],"content":"历史 Docker Daemon从最初集成在docker命令中（1.11版本前）， 独立成单独二进制程序（1.11版本开始）2016.12 containerd是容器技术标准化之后的产物，为了能够兼容OCI标准，从Docker Daemon剥离 containerd分v0.2.x ， v1.x版本 v0.2.x 由docker daemon集成 v1.x 开始支持oci标准 containerd项目地址： github.com/docker/containerd containerd捐献给cncf 2017.03 项目地址修改为 github.com/containerd/containerd v1.0.0正式版发布 2017.12 支持OCI接口 v1.1.0正式版 2018.4 支持CRI接口 v1.2.0正式版 2018.10 runtime v2稳定 shim shim是一个真实运行的容器的真实载体 ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:1:0","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"架构 组件如图 ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:2:0","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"glossary 服务： 提供grpc服务 sandbox：一种特殊容器，主要作用准备容器隔离的环境，包括网络，io目录，cgroup parent container：业务容器，每个进程都由一个task 管理 shim：管理器(containerd) 和 runtime的粘和层，因为runc是二进制文件执行，无法常驻内存，使用shim一方面方便管理，另一方面扩展runtime; shim管理 sandbox + container(s) task：代表进程+io runtime v2: 可以理解 task 管理器 ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:3:0","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"服务 每个服务都是plugin方式加入，包括不限于image, runtime, grpc server等 plugin特点 插件URI为 Type+ID，如 io.containerd.snapshotter.v1.overlayfs，io.containerd.snapshotter.v1是Type，overlayfs是ID 插件类型之间有依赖, 从下向上排序： ContentPlugin， SnapshotPlugin MetadataPlugin GCPlugin，RuntimePlugin，DiffPlugin ServicePlugin InternalPlugin，GRPCPlugin 数据结构 ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:4:0","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"diff 用于snaphost的操作 diff/walking/plugin目录， DiffPlugin类型，依赖MetadataPlugin 提供两个方法 Compare 根据提供的 lower，upper 信息，得到oscispec.Descriptor Apply 从镜像压缩文件中读取内容，并按序执行多个处理器，主要是校验，解压，自定义插件等 ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:4:1","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"scheduler 调度器，gc/scheduler目录， GCPlugin类型，依赖MetadataPlugin 配置信息 PauseThreshold: 暂停阈值，平均每秒2%的暂停时间，使用该值计算下次等待周期，默认为0.02, 最大值0.5 DeletionThreshold：删除阈值，在删除多少次之后至少保证发生gc调度，0表示不会被删除触发，默认0 MutationThreshold： 突变阈值 ScheduleDelay：调度延迟，当发生手动触发或者阈值触发后，延迟多久才开始GC，默认0ms StartupDelay： 慢启动，默认100ms goroutine gc实际是执行content.GarbageCollect ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:4:2","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"monitor 容器的起停状态同步， InternalPlugin类型，依赖ServicePlugin goroutine 过滤标签 containerd.io/restart.status的container 对比container和预期状态是否相符 重新执行start或stop ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:4:3","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"cgroup 进程oom监听和指标统计，在 metrics/cgroup 目录， TaskMonitorPlugin类型 作用：task的指标和oom监听 主要是shim 在使用该模块，shim需要监听 task 是否发生OOM ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:4:4","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"runtime v1 容器进程管理器,， RuntimePlugin类型，依赖MetadataPlugin 配置 shim： shim二进制文件地址 runtime： 被shim使用的runtime二进制文件地址 no_shim： 直接调用runtime，跳过shim shim_debug： 是否打开shim调试 ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:4:5","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"runtime v2 容器进程管理器, RuntimePluginV2类型，依赖MetadataPlugin 在client中被配置为默认的runtime 初始化时会执行以下内容 创建目录 {root/state}/io.containerd.runtime.v2.task 读取 {state}/io.containerd.runtime.v2.task目录 恢复shim 管理 tips： fifo文件ro打开：当无写端，若是nonblocking模式，则返回，否则阻塞 fifo文件wo打开：当无读端，返回ENXIO，若读端关闭时写数据，则返回SIGPIPE fifo文件rw打开：总是成功 ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:4:6","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"snapshot 容器的rootfs管理器，SnapshotPlugin类型 linux 平台 snapshots/{native, overlay, devmapper，btrfs} containerd/{aufs,zfs} windows平台 snapshots/{lcow, windows} # man 8 mount # references: https://wiki.archlinux.org/title/Overlay_filesystem tips: 1 overlay 挂载 mount -t overlay overlay -o lowerdir=/lower:/lower2,upperdir=/upper,workdir=/work /merged // upperdir 一般是可写层 // workdir 空目录， 2 devmapper 挂载 mount -t ext4 /dev/mapper/xx {target} 3 native 挂载 mount -t bind {src} {dst} 4 btrfs mount -t btrfs ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:4:7","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"services 提供rpc服务和数据库操作等 services/目录下，具体有以下的内容 ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:5:0","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"container/image/lease/namespace local.go: 是service Plugin类型，对数据库的操作，以及event的发送 service.go: 是grpc Plugin类型，提供grpc服务，并使用local内相同接口 ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:5:1","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"content service.go: 提供grpc服务，CRUD content数据 contentserver : content具体操作(content) ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:5:2","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"task local.go: 调用runtime v2同名接口，包括容器创建，开始，结束，删除，终端，状态等信息 server.go: 提供grpc服务 ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:5:3","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"diff local.go: 使用diff插件 service.go: 提供grpc服务 ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:5:4","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"event service.go: 提供grpc服务，包括转发，推送，订阅功能 ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:5:5","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"introspection service.go: 提供grpc服务，列举注册的plugin ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:5:6","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"healthcheck service.go: 提供 grpc服务，检查指定服务的健康 ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:5:7","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"version service.go: 提供 grpc服务，检查containerd version ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:5:8","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"opt 配置目录 root 添加 {root}/bin 到环境变量 PATH中 添加 {root}/lib 到环境变量LD_LIBRARY_PATH中 ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:5:9","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"==server== 基础plugin，扩展插件的初始化和服务启动 流程 注册 ContentPlugin插件 注册 MetadataPlugin插件 注册 proxy插件(ContentPlugin或SnapshotPlugin类型) 注册DiffPlugin插件 (stream processor类型)，目前主要是处理 加密的镜像 监听event类型 /tasks/oom /images/ version = 2 root = \"/var/lib/mycontainerd\" #持久化目录 state = \"/var/run/mycontainerd\" #非持久化目录 oom_score = 0 # containerd的oom分数 ... [proxy_plugins] [proxy_plugins.stargz] type = \"snapshot\" #支持snapshot和content类型 address = \"/run/containerd-stargz-grpc/containerd-stargz-grpc.sock\" [stream_processors] [stream_processors.\"io.containerd.ocicrypt.decoder.v1.tar.gzip\"] accepts = [\"application/vnd.oci.image.layer.v1.tar+gzip+encrypted\"] returns = \"application/vnd.oci.image.layer.v1.tar+gzip\" path = \"/usr/local/bin/ctd-decoder\" [cgroup] path = \"\" # containerd 所在路径，去除 ‘/sys/fs/cgroup/'前缀的 [plugins] [plugins.\"io.containerd.gc.v1.scheduler\"] pause_threshold = 0.02 deletion_threshold = 0 mutation_threshold = 100 schedule_delay = \"0s\" startup_delay = \"100ms\" [plugins.\"io.containerd.internal.v1.restart\"] interval = \"10s\" #restart插件，遍历container的周期 [plugins.\"io.containerd.metadata.v1.bolt\"] content_sharing_policy = \"shared\" #content策略，shared则所有不同ns的都可以访问，另一种是isolated [plugins.\"io.containerd.monitor.v1.cgroups\"] no_prometheus = false [plugins.\"io.containerd.runtime.v1.linux\"] shim = \"containerd-shim\" runtime = \"runc\" runtime_root = \"\" no_shim = false shim_debug = false [plugins.\"io.containerd.runtime.v2.task\"] platforms = [\"linux/amd64\"] #镜像支持的平台类型 [plugins.\"io.containerd.service.v1.diff-service\"] default = [\"walking\"] #walking是内置的diff插件 ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:5:10","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"cri 提供grpc 服务，GRPCPlugin类型，依赖ServicePlugin cri 常见配置 [plugins.cri] disable_tcp_service = true stream_server_address = \"127.0.0.1\" # unix 监听地址 stream_server_port = \"0\" # unix 监听端口,0是随机分配 stream_idle_timeout = \"4h0m0s\" #idle连接超时时间 enable_selinux = false sandbox_image = \"hub.easystack.io/captain/pause-amd64:3.0\" stats_collect_period = 10 #单位秒，snapshot状态收集 systemd_cgroup = false #只适用io.containerd.runtime.v1.linux enable_tls_streaming = false # 是否支持tls max_container_log_line_size = 16384 #单行日志最大byte disable_cgroup = false # 当rootless 运行时，需要设置为true disable_apparmor = false restrict_oom_score_adj = false netns_mounts_under_state_dir = false #将net ns设置到{state}/netns目录下,而不是默认的/run/netns下 max_concurrent_downloads = 3 unset_seccomp_profile = \"\" #seccomp配置文件 disable_proc_mount = false #禁用k8s ProcMount， 对于k8s \u003c= 1.11时，必须设置为true tolerate_missing_hugetlb_controller = true #1.5+,对于k8s \u003c1.19,不支持配置hugetlb disable_hugetlb_controller = true #1.5+, 适用于rootless+cgroupv2+systemd ignore_image_defined_volumes = true #1.5+，创建container时，忽略image中定义的volume [plugins.cri.x509_key_pair_streaming] #stream tls配置 tls_cert_file = \"\" tls_key_file = \"\" [plugins.cri.image_decryption] # 解密镜像，参考项目 github.com/containerd/imgcrypt key_model = none [plugins.cri.containerd] snapshotter = \"overlayfs\" #使用何种snapshot default_runtime_name = \"runc\" #默认runtime，必须出现在后续的列表中 no_pivot = false #只适用io.containerd.runtime.v1.linux disable_snapshot_annotations = false #stargz snapshot需要配置 discard_unpacked_layers = false #当执行content GC时，是否回收已经unpack的content [plugins.cri.containerd.default_runtime] #被弃用 privileged_without_host_devices = false # 设置特权模式下时，是否映射主机上设备到容器内 [plugins.cri.containerd.untrusted_workload_runtime] #被弃用 privileged_without_host_devices = false [plugins.cri.containerd.runtimes] [plugins.cri.containerd.runtimes.runc] runtime_type = \"io.containerd.runc.v2\" #将被用于解析为具体二进制文件名 [plugins.cri.containerd.runtimes.runc.options] # NoPivotRoot disables pivot root when creating a container. NoPivotRoot = false # NoNewKeyring disables new keyring for the container. NoNewKeyring = false # 设置shim cgroup目录，默认是和containerd一起 ShimCgroup = \"\" # 设置shim log user id. IoUid = 0 # 设置shim log group id. IoGid = 0 # 设置二进制文件名，否则将使用type字段 BinaryName = \"\" # runc的目录，默认为 {state}/runc. Root = \"\" # criu 二进制文件路径. CriuPath = \"\" # 使用cgroup的systemd 驱动，注意需要和kubelet中同样设置 SystemdCgroup = true # CriuImagePath is the criu image path CriuImagePath = \"\" # CriuWorkPath is the criu work path. CriuWorkPath = \"\" privileged_without_host_devices = false [plugins.cri.containerd.runtimes.gvisor] runtime_type = \"io.containerd.gvisor.v2\" pod_annotations =[\"\"] container_annotations = [\"\"] base_runtime_spec=\"\" # 文件绝对路径，且内容为oci runtime文件格式 [plugins.cri.containerd.runtimes.firecracker] runtime_type = \"io.containerd.firecracker.v2\" [plugins.cri.cni] bin_dir = \"/opt/cni/bin\" conf_dir = \"/etc/cni/net.d\" max_conf_num = 1 #若有多个cni配置文件，则按照此值调用多个cni配置文件 [plugins.cri.registry] config_path= /xxx # 目录，若配置该值，则以下内容被忽略 [plugins.cri.registry.configs] #被弃用，使用config_path代替，在1.7后被移除 [plugins.cri.registry.configs.\"hub.easystack.io\"] [plugins.cri.registry.configs.\"hub.easystack.io\".tls] insecure_skip_verify = true [plugins.cri.registry.mirrors] #被弃用，使用config_path代替，在1.7后被移除 [plugins.cri.registry.mirrors.\"docker.io\"] endpoint = [\"https://registry-1.docker.io\"] [plugins.cri.registry.mirrors.\"hub.easystack.io\"] endpoint = [\"https://hub.easystack.io\"] ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:6:0","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"},{"categories":["container"],"content":"仓库配置 配置中会逐渐废弃单一文件配置方式，这里描述config_path config_path= /opt/mycontainerd/hosts # 目录内容 . ├── docker.io │ └── hosts.toml └── k8s.gcr.io └── hosts.toml # hosts.toml内容 server = \"https://k8s.gcr.io\" [host.\"https://k8s.gcr.io\"] skip_verify = true capabilities = [ \"pull\", \"resolve\", \"push\" ] ca: \"\" #文件路径或文件内容 ","date":"2020-12-09","objectID":"/2020/12/containerd-1/:6:1","tags":["containerd","go"],"title":"Containerd 简介1","uri":"/2020/12/containerd-1/"}]